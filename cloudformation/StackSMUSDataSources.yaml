AWSTemplateFormatVersion: 2010-09-09
Description: "Guidance to create demo data sources for Amazon SageMaker Unified Studio"
Parameters:

  RDSPort:
    Type: Number
    Default: 5432
    Description: The port Aurora PostgreSQL is listening on

  RDSEngineVersion:
    Type: String
    Default: "16.4"
    Description: The Aurora PostgreSQL engine version

  SecurityGroupId:
    Type: AWS::EC2::SecurityGroup::Id
    Description: The Project's SecurityGroupId

  VpcId:
    Type: AWS::EC2::VPC::Id
    Description: VpcId associated with the SageMaker Domain

  SubnetIds:
    Type: List<AWS::EC2::Subnet::Id>
    Description: PublicSubnet1

  AmazonDataZoneProject:
    Type: String
    Description: The Project ID of the Project in Amazon SageMaker Unified Studio
    
  ProjectRoleARN:
    Type: String
    Description: The IAM Role ARN of the Project in Amazon SageMaker Unified Studio

Resources:

  RDSDBSubnetGroup:
    Type: "AWS::RDS::DBSubnetGroup"
    UpdateReplacePolicy: "Retain"
    DeletionPolicy: "Delete"
    Properties:
      DBSubnetGroupDescription: "Amazon RDS Subnet Group"
      DBSubnetGroupName: !Sub "default-${VpcId}"
      SubnetIds: !Ref SubnetIds

  RDSCluster:
    Type: "AWS::RDS::DBCluster"
    UpdateReplacePolicy: "Retain"
    DeletionPolicy: "Delete"
    DependsOn: RDSDBSubnetGroup
    Properties:
      DBClusterIdentifier: demo-cluster
      ManageMasterUserPassword: true
      MasterUsername: "postgres"
      Engine: "aurora-postgresql"
      EngineVersion: !Ref RDSEngineVersion
      EnableHttpEndpoint: false
      Port: !Ref RDSPort
      DBSubnetGroupName: !Ref RDSDBSubnetGroup
      DBClusterParameterGroupName: !Ref ZeroETLDBClusterParameterGroup
      VpcSecurityGroupIds:
        - !Ref SecurityGroupId
      Tags:
        - Key: "AmazonDataZoneProject"
          Value: !Ref AmazonDataZoneProject
      
  RDSDBInstance:
    Type: "AWS::RDS::DBInstance"
    UpdateReplacePolicy: "Retain"
    DeletionPolicy: "Delete"
    DependsOn: RDSCluster
    Properties:
      DBInstanceIdentifier: demo-instance
      #DBName: "postgres"
      Engine: "aurora-postgresql"
      DBClusterIdentifier: !Ref RDSCluster
      PubliclyAccessible: true
      DBInstanceClass: "db.t4g.medium"
      MonitoringInterval: 0
      Tags:
        - Key: "AmazonDataZoneProject"
          Value: !Ref AmazonDataZoneProject

  RedshiftAdminSecret:
    Type: AWS::SecretsManager::Secret
    UpdateReplacePolicy: "Retain"
    DeletionPolicy: "Delete"
    Properties:
      Description: AutoGenerated secret for the admin for Redshift Serverless
      #Only printable ASCII characters except for '/', '@', '"', ' ', '\', ''' may be used.
      GenerateSecretString:
        SecretStringTemplate: '{"username": "admin"}'
        GenerateStringKey: password
        PasswordLength: 8
        IncludeSpace: false
        ExcludePunctuation: true
        ExcludeCharacters: '"@/\'
      Tags:
        - Key: AmazonDataZoneProject
          Value: !Ref AmazonDataZoneProject

  RedshiftServerlessNS:
    Type: AWS::RedshiftServerless::Namespace
    UpdateReplacePolicy: "Retain"
    DeletionPolicy: "Delete"
    DependsOn: RedshiftAdminSecret
    Properties:
      NamespaceName: "demo-ns"
      DbName: "dev"
      DefaultIamRoleArn: !Ref ProjectRoleARN
      IamRoles:
        - !Ref ProjectRoleARN
      ManageAdminPassword: false
      AdminUsername: !Join [ '', [ '{{resolve:secretsmanager:', !Ref RedshiftAdminSecret, ':SecretString:username}}' ] ]
      AdminUserPassword: !Join [ '', [ '{{resolve:secretsmanager:', !Ref RedshiftAdminSecret, ':SecretString:password}}' ] ]
      Tags:
        - Key: AmazonDataZoneProject
          Value: !Ref AmazonDataZoneProject
      NamespaceResourcePolicy:
        Version: "2012-10-17"
        Statement:
        - Effect: "Allow"
          Principal:
            Service: "redshift.amazonaws.com"
          Action: "redshift:AuthorizeInboundIntegration"
          Condition:
            StringEquals:
              aws:SourceArn: !GetAtt RDSCluster.DBClusterArn
        - Effect: "Allow"
          Principal:
            AWS: !Sub "arn:aws:iam::${AWS::AccountId}:root"
          Action: "redshift:CreateInboundIntegration"

  RedshiftServerlessWG:
    Type: AWS::RedshiftServerless::Workgroup
    DependsOn: RedshiftServerlessNS
    UpdateReplacePolicy: "Retain"
    DeletionPolicy: "Delete"
    Properties:
      WorkgroupName: "demo-wg"
      NamespaceName: !Ref RedshiftServerlessNS
      SecurityGroupIds:
        - !Ref SecurityGroupId
      SubnetIds: !Ref SubnetIds
      BaseCapacity: 8
      MaxCapacity: 512
      PubliclyAccessible: false
      ConfigParameters:
        - ParameterKey: enable_case_sensitive_identifier
          ParameterValue: true
      Tags:
        - Key: AmazonDataZoneProject
          Value: !Ref AmazonDataZoneProject

  ZeroETLDBClusterParameterGroup:    
    Type: AWS::RDS::DBClusterParameterGroup
    UpdateReplacePolicy: "Retain"
    DeletionPolicy: "Delete"
    Properties:
      DBClusterParameterGroupName: "zetl-dbcluster-params"
      Description: Aurora Cluster Parameter Group to support Zero-ETL
      Family: aurora-postgresql16
      Parameters:
        aurora.logical_replication_globaldb: 0
        aurora.logical_replication_backup: 0
        aurora.enhanced_logical_replication: 1
        rds.logical_replication: 1

  ZeroETLPG2Redshift:
    Type: AWS::RDS::Integration
    UpdateReplacePolicy: "Retain"
    DeletionPolicy: "Delete"
    DependsOn: ["RDSDBInstance","RedshiftServerlessWG"]
    Properties:
      DataFilter: "include: postgres.public.*"
      Description: "Zero-ETL integration between Aurora PostgreSQL and Redshift to replicate retail data"
      IntegrationName: "zetl-aurorapg"
      SourceArn: !GetAtt RDSCluster.DBClusterArn
      TargetArn: !GetAtt RedshiftServerlessNS.Namespace.NamespaceArn
      Tags:
        - Key: AmazonDataZoneProject
          Value: !Ref AmazonDataZoneProject
   
  DynamoDB:
    Type: AWS::DynamoDB::Table
    UpdateReplacePolicy: "Retain"
    DeletionPolicy: "Delete"
    Properties:
      TableName: invoices
      AttributeDefinitions:
        - AttributeName: customer_id
          AttributeType: N
        - AttributeName: invoice_number
          AttributeType: S
      BillingMode: PAY_PER_REQUEST
      KeySchema:
        - AttributeName: customer_id
          KeyType: HASH 
        - AttributeName: invoice_number
          KeyType: RANGE
      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: true
      Tags:
        - Key: AmazonDataZoneProject
          Value: !Ref AmazonDataZoneProject
      ResourcePolicy:
        PolicyDocument:
          Version: "2012-10-17"
          Statement:
          - Condition:
              StringEquals:
                aws:SourceAccount: !Sub "${AWS::AccountId}"
              ArnEquals:
                aws:SourceArn: !Sub "arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:integration:*"
            Resource: !Sub "arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/invoices"
            Action:
            - "dynamodb:ExportTableToPointInTime"
            - "dynamodb:DescribeTable"
            Effect: "Allow"
            Principal:
              Service: "redshift.amazonaws.com"
          - Condition:
              StringEquals:
                aws:SourceAccount: !Sub "${AWS::AccountId}"
              ArnEquals:
                aws:SourceArn: "arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:integration:*"
            Resource: !Sub "arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/invoices/export/*"
            Action: "dynamodb:DescribeExport"
            Effect: "Allow"
            Principal:
              Service: "redshift.amazonaws.com"
          - Condition:
              ArnEquals:
                aws:PrincipalArn:
                - !Ref ProjectRoleARN
            Resource: !Sub "arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/invoices"
            Action:
            - "dynamodb:Query"
            - "dynamodb:Scan"
            - "dynamodb:DescribeTable"
            - "dynamodb:PartiQLSelect"
            Effect: "Allow"
            Principal: "*"
            Sid: "misc"

  LambdaDDBInsert:
    Type: 'AWS::Lambda::Function'
    UpdateReplacePolicy: "Retain"
    DeletionPolicy: "Delete"
    DependsOn: DynamoDB
    Properties:
      Runtime: python3.13
      Handler: index.handler
      Role: !GetAtt LambdaDDBInsertRole.Arn
      Environment:
        Variables:
          tableName: !Ref DynamoDB
      Code:
        ZipFile: |
            import json
            import boto3
            import os
            import urllib3
            import logging
            from typing import Dict, Any

            # Set up logging
            logger = logging.getLogger()
            logger.setLevel(logging.INFO)

            # Initialize DynamoDB client
            dynamodb = boto3.resource('dynamodb')
            http = urllib3.PoolManager()

            def send_response(event: Dict[str, Any], context: Any, response_status: str, response_data: Dict[str, Any]) -> None:
                """Send a response to CloudFormation"""
                response_body = {
                    'Status': response_status,
                    'Reason': f'See details in CloudWatch Log Stream: {context.log_stream_name}',
                    'PhysicalResourceId': context.log_stream_name,
                    'StackId': event['StackId'],
                    'RequestId': event['RequestId'],
                    'LogicalResourceId': event['LogicalResourceId'],
                    'Data': response_data
                }
                
                logger.info(f'Response body: {json.dumps(response_body)}')
                
                response_url = event['ResponseURL']
                
                try:
                    response = http.request(
                        'PUT',
                        response_url,
                        body=json.dumps(response_body).encode('utf-8'),
                        headers={'Content-Type': 'application/json'}
                    )
                    logger.info(f'CloudFormation response status code: {response.status}')
                    
                except Exception as e:
                    logger.error(f'Failed to send response to CloudFormation: {str(e)}')
                    raise

            def handler(event: Dict[str, Any], context: Any) -> None:
                """Lambda function handler"""
                try:
                    # Get table name from environment variable
                    table_name = os.environ.get('tableName')
                    if not table_name:
                        raise ValueError('tableName environment variable is required')
                    
                    logger.info(f'Starting to seed table: {table_name}')
                    table = dynamodb.Table(table_name)
                    
                    # Items to add
                    items_to_add = [
                        {'customer_id': 1, 'invoice_number': 'INV-001', 'total': 200},
                        {'customer_id': 2, 'invoice_number': 'INV-002', 'total': 200},
                        {'customer_id': 3, 'invoice_number': 'INV-003', 'total': 200},
                        {'customer_id': 4, 'invoice_number': 'INV-004', 'total': 200},
                        {'customer_id': 5, 'invoice_number': 'INV-005', 'total': 200}
                    ]
                    
                    # Use batch_writer for better performance
                    with table.batch_writer() as batch:
                        for item in items_to_add:
                            batch.put_item(Item=item)
                            logger.info(f'Added item with customer_id: {item["customer_id"]}')
                    
                    logger.info(f'Successfully added {len(items_to_add)} items to {table_name}')
                    
                    # Send success response to CloudFormation
                    send_response(
                        event, 
                        context, 
                        'SUCCESS',
                        {'Result': f'Successfully added {len(items_to_add)} items to DynamoDB table'}
                    )
                    
                except Exception as e:
                    logger.error(f'Error: {str(e)}')
                    # Send failure response to CloudFormation
                    send_response(
                        event,
                        context,
                        'FAILED',
                        {'Error': str(e)}
                    )
                    raise
      Timeout: 30
  LambdaDDBInsertRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
        - 'arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess'
  RunLambdaDDBInsert:
    Type: 'Custom::MyCustomResource'
    Properties:
      ServiceToken: !GetAtt LambdaDDBInsert.Arn      
          
Outputs:
  AuroraPGHost:
    Description: The Aurora PostgreSQL host
    Value: !GetAtt RDSCluster.Endpoint.Address
    
  AuroraPGPort:
    Description: The Aurora PostgreSQL port
    Value: !GetAtt RDSCluster.Endpoint.Port
    
  AuroraPGDB:
    Description: The Aurora PostgreSQL database name.
    Value: "postgres"
    
  AuroraPGSecretArn:
    Description: The Aurora PostgreSQL secret arn
    Value: !GetAtt RDSCluster.MasterUserSecret.SecretArn

  RedshiftHost:
    Description: The Redshift Serverless host
    Value: !GetAtt RedshiftServerlessWG.Workgroup.Endpoint.Address

  RedshiftPort:
    Description: The Redshift Serverless port
    Value: "5439"
    #!GetAtt RedshiftServerlessWG.Workgroup.Endpoint.Port

  RedshiftDB:
    Description: The Redshift Serverless database name
    Value: !GetAtt RedshiftServerlessNS.Namespace.DbName
    
  RedshiftSecretArn:
    Description: The Redshift Serverless secret arn
    Value: !Ref RedshiftAdminSecret